{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import prep_plays\n",
    "import wrangle_plays_data\n",
    "from scipy import stats\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_plays.prep_plays_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pass_stopped.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.playDescription.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validate, y_validate, X_test, y_test = wrangle_plays_data.train_validate_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_validate_scaled, X_test_scaled = wrangle_plays_data.min_max_scale(X_train, X_validate, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.QB_under_pressure.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(500, 20, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crosstab of data I want to visualize\n",
    "crosstab = pd.crosstab(df.pass_stopped, df.QB_under_pressure)\n",
    "# Visualizes heatmap to see differences in values\n",
    "sns.heatmap(crosstab, annot= True, cmap= 'Reds', fmt= 'd')\n",
    "\n",
    "plt.title('Passes by QB Under Pressure')\n",
    "locs, labels = plt.yticks()\n",
    "plt.yticks(locs, ('Not Stopped', 'Stopped'))\n",
    "plt.xticks(locs, ('Not Under Pressure', 'Under Pressure'))\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incompletion_rate = (((df.pass_stopped == 1).sum()) / (df.pass_stopped.count())).round(2)\n",
    "incompletion_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the questions\n",
    "features = ['QB_under_pressure','dime', 'four_three','three_four', 'nickel']\n",
    "\n",
    "# subplot each questions side by side\n",
    "# adding a line to measure where average autism rating is\n",
    "_, ax = plt.subplots(nrows=1, ncols=5, figsize=(20, 4), sharey=True)\n",
    "for i, feature in enumerate(features):\n",
    "    sns.barplot(feature, 'pass_stopped', data=df, ax=ax[i], alpha=.8)\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_ylabel('Incomplete Rate')\n",
    "    ax[i].set_title(feature)\n",
    "    ax[i].axhline(incompletion_rate, ls='--', color='grey')\n",
    "print('Comparing If Schemes And QB Pressure Impact Incompletion Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Dime and Nickel have a closer to 50/50 chance of stopping the pass over 3-4 and 4-3 defensive schemes based on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0: There is no difference in stopped passes between nickel defense and average nickel defense\n",
    "\n",
    "Ha: There is a difference in stopped passes between nickel defense and average nickel defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha= .05\n",
    "stopped_pass = df[df.pass_stopped == 1]\n",
    "t, p = stats.ttest_1samp(stopped_pass.nickel, df.nickel.mean())\n",
    "\n",
    "print(f't = {t:.2f}')\n",
    "print(f'p = {p:.90f}')\n",
    "print(f'Our p-value is less than our alpha: {p < alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0: There is no difference in stopped passes between dime defense and average dime defense\n",
    "\n",
    "Ha: There is a difference in stopped passes between dime defense and average dime defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha= .05\n",
    "stopped_pass = df[df.pass_stopped == 1]\n",
    "t, p = stats.ttest_1samp(stopped_pass.dime, df.dime.mean())\n",
    "\n",
    "print(f't = {t:.2f}')\n",
    "print(f'p = {p:.90f}')\n",
    "print(f'Our p-value is less than our alpha: {p < alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0: There is no difference in stopped passes between 3-4 defense and average 3-4 defense\n",
    "\n",
    "Ha: There is a difference in stopped passes between 3-4 defense and average 3-4 defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha= .05\n",
    "stopped_pass = df[df.pass_stopped == 1]\n",
    "t, p = stats.ttest_1samp(stopped_pass.three_four, df.three_four.mean())\n",
    "\n",
    "print(f't = {t:.2f}')\n",
    "print(f'p = {p:.90f}')\n",
    "print(f'Our p-value is less than our alpha: {p < alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0: There is no difference in stopped passes between 4-3 defense and average 4-3 defense\n",
    "\n",
    "Ha: There is a difference in stopped passes between 4-3 defense and average 4-3 defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha= .05\n",
    "stopped_pass = df[df.pass_stopped == 1]\n",
    "t, p = stats.ttest_1samp(stopped_pass.four_three, df.four_three.mean())\n",
    "\n",
    "print(f't = {t:.2f}')\n",
    "print(f'p = {p:.90f}')\n",
    "print(f'Our p-value is less than our alpha: {p < alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0: A defense stopping a pass is independent of QB pressure\n",
    "\n",
    "Ha: A defense stopping a pass is not independent of QB pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(df.pass_stopped, df.QB_under_pressure)\n",
    "Chi2, p, degf, expected = stats.chi2_contingency(crosstab)\n",
    "print(f'Our p-value is {p:.90f}.')\n",
    "print(f'Our p-value is less than our alpha: {p < alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H0: A defense stopping a pass is independent of which down it is\n",
    "\n",
    "Ha: A defense stopping a pass is not independent of which down it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(df.pass_stopped, df.down)\n",
    "Chi2, p, degf, expected = stats.chi2_contingency(crosstab)\n",
    "print(f'Our p-value is {p:.90f}.')\n",
    "print(f'Our p-value is less than our alpha: {p < alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions I will need for modeling\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish our baseline\n",
    "baseline = ((df.pass_stopped == 1).sum() / df.pass_stopped.count()).round(2)\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assessing the RFE using a linear regression model\n",
    "lr =  LogisticRegression(random_state=123)\n",
    "\n",
    "rfe = RFE(lr, 10)\n",
    "lm_X_rfe_train = rfe.fit_transform(X_train_scaled,y_train)\n",
    "lm_X_rfe_val = rfe.transform(X_validate_scaled)\n",
    "lm_X_rfe_test = rfe.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we want to filter through and return only the best features\n",
    "mask = rfe.support_ \n",
    "rfe_features = X_train_scaled.columns[mask]\n",
    "print(f'selected {len(rfe_features)} features:', ', '.join(rfe_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign random forest to a variable\n",
    "def random_forest(leaf, depth, estimator):\n",
    "    rf = RandomForestClassifier(bootstrap=True, \n",
    "                            class_weight=None, \n",
    "                            criterion='gini',\n",
    "                            min_samples_leaf=leaf,\n",
    "                            n_estimators=estimator,\n",
    "                            max_depth=depth, \n",
    "                            random_state=123)\n",
    "    \n",
    "    print('---------------------------- Train -------------------------------')\n",
    "    \n",
    "    # fit train data\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    # assign predicitons\n",
    "    y_pred = rf.predict(X_train_scaled)\n",
    "    # assign probabilities\n",
    "    y_pred_proba = rf.predict_proba(X_train_scaled)\n",
    "    print('Accuracy of random forest classifier on training set: {:.2f}'\n",
    "         .format(rf.score(X_train_scaled, y_train)))\n",
    "    print('Training Data Matrix')\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "    # print report\n",
    "    print('Training Data Report')\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    \n",
    "    print('---------------------------- Validate -------------------------------')\n",
    "    \n",
    "    # assign predicitions\n",
    "    y_pred = rf.predict(X_validate_scaled)\n",
    "    # assign probabilities\n",
    "    y_pred_proba = rf.predict_proba(X_validate_scaled)\n",
    "    print('Accuracy of random forest classifier on validate set: {:.2f}'\n",
    "         .format(rf.score(X_validate_scaled, y_validate)))\n",
    "    print('Training Data Matrix')\n",
    "    print(confusion_matrix(y_validate, y_pred))\n",
    "    # print report\n",
    "    print('Training Data Report')\n",
    "    print(classification_report(y_validate, y_pred))\n",
    "    \n",
    "    print('--------------------- Important Features ---------------------------')\n",
    "    feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_train_scaled.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "    return random_forest, feature_importances\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(6, 12, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(8, 15, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrangle.py Loaded Successfully\n",
      "Acquire.py Loaded Successfully\n",
      "Prep.py Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "import MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- Train -------------------------------\n",
      "Accuracy of random forest classifier on training set: 0.91\n",
      "Training Data Matrix\n",
      "[[4085  521]\n",
      " [  93 2441]]\n",
      "Training Data Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93      4606\n",
      "           1       0.82      0.96      0.89      2534\n",
      "\n",
      "    accuracy                           0.91      7140\n",
      "   macro avg       0.90      0.93      0.91      7140\n",
      "weighted avg       0.92      0.91      0.92      7140\n",
      "\n",
      "---------------------------- Validate -------------------------------\n",
      "Accuracy of random forest classifier on validate set: 0.88\n",
      "Training Data Matrix\n",
      "[[2724  439]\n",
      " [ 137 1460]]\n",
      "Training Data Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90      3163\n",
      "           1       0.77      0.91      0.84      1597\n",
      "\n",
      "    accuracy                           0.88      4760\n",
      "   macro avg       0.86      0.89      0.87      4760\n",
      "weighted avg       0.89      0.88      0.88      4760\n",
      "\n",
      "---------------------------- Test -------------------------------\n",
      "Accuracy of random forest classifier on validate set: 0.88\n",
      "Training Data Matrix\n",
      "[[2815  470]\n",
      " [ 149 1667]]\n",
      "Training Data Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90      3285\n",
      "           1       0.78      0.92      0.84      1816\n",
      "\n",
      "    accuracy                           0.88      5101\n",
      "   macro avg       0.86      0.89      0.87      5101\n",
      "weighted avg       0.89      0.88      0.88      5101\n",
      "\n",
      "--------------------- Important Features ---------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random_forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a3ef313e0b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMVP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMVP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/codeup-data-science/NFL-Big-Data-Bowl-2021/MVP.py\u001b[0m in \u001b[0;36mMVP\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m                                    \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                     columns=['importance']).sort_values('importance',ascending=False)\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrandom_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_forest' is not defined"
     ]
    }
   ],
   "source": [
    "MVP.MVP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
